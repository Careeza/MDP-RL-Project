{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from typing import Callable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Card = int\n",
    "Hand = torch.Tensor\n",
    "Hands = torch.Tensor\n",
    "Action = int\n",
    "Actions = torch.Tensor\n",
    "Policy = Callable[[Hand, Hand], Action]\n",
    "MultiPolicy = Callable[[Hands, Hands], Actions]\n",
    "Deck = torch.Tensor\n",
    "Decks = torch.Tensor\n",
    "\n",
    "ACE: Card = 1\n",
    "TWO: Card = 2\n",
    "THREE: Card = 3\n",
    "FOUR: Card = 4\n",
    "FIVE: Card = 5\n",
    "SIX: Card = 6\n",
    "SEVEN: Card = 7\n",
    "EIGHT: Card = 8\n",
    "NINE: Card = 9\n",
    "TEN: Card = 10\n",
    "JACK: Card = 10\n",
    "QUEEN: Card = 10\n",
    "KING: Card = 10\n",
    "\n",
    "card_values: list[Card] = [ACE, TWO, THREE, FOUR, FIVE, SIX, SEVEN, EIGHT, NINE, TEN, JACK, QUEEN, KING]\n",
    "card_values = torch.tensor(card_values, dtype=torch.float32)\n",
    "num_cards: int = len(card_values)\n",
    "cards: list[int] = list(range(num_cards))\n",
    "cards = torch.tensor(cards, dtype=torch.int64)\n",
    "\n",
    "max_card_value: int = max(card_values)\n",
    "max_score: int = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIT: Action = 0 \n",
    "STAND: Action = 1\n",
    "DOUBLE: Action = 2\n",
    "SPLIT: Action = 3\n",
    "SURRENDER: Action = 4\n",
    "\n",
    "actions_str: dict[Action, str] = {\n",
    "    HIT: 'Hit',\n",
    "\tSTAND: 'Stand',\n",
    "\tDOUBLE: 'Double',\n",
    "\tSPLIT: 'Split',\n",
    "\tSURRENDER: 'Surrender',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 7\n",
    "max_card_per_hand = 4 * N\n",
    "hand_played_per_simulation = 100\n",
    "nb_simulation = 5\n",
    "ratio_mean = 1 / nb_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minScore(ace=0, two=0, three=0, four=0, five=0, six=0, seven=0, eight=0, nine=0, ten=0, jack=0, queen=0, king=0) -> int:\n",
    "\t\"\"\"\n",
    "\tScore of some cards\n",
    "\tEach ace is counted as 11 if the total score is less than or equal to 21, otherwise it is counted as 1\n",
    "\t\"\"\"\n",
    "\tscore = 0\n",
    "\tscore += ace * ACE\n",
    "\tscore += two * TWO\n",
    "\tscore += three * THREE\n",
    "\tscore += four * FOUR\n",
    "\tscore += five * FIVE\n",
    "\tscore += six * SIX\n",
    "\tscore += seven * SEVEN\n",
    "\tscore += eight * EIGHT\n",
    "\tscore += nine * NINE\n",
    "\tscore += ten * TEN\n",
    "\tscore += jack * JACK\n",
    "\tscore += queen * QUEEN\n",
    "\tscore += king * KING\n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_hands_to_index: dict[Hand, int] = {}\n",
    "idx = 0\n",
    "for ace in range(0, max_score+1):\n",
    "\tscore = minScore(ace)\n",
    "\tfor two in range(0, (max_score-score) // TWO +1):\n",
    "\t\tscore = minScore(ace, two)\n",
    "\t\tfor three in range(0, (max_score-score) // THREE +1):\n",
    "\t\t\tscore = minScore(ace, two, three)\n",
    "\t\t\tfor four in range(0, (max_score-score) // FOUR +1):\n",
    "\t\t\t\tscore = minScore(ace, two, three, four)\n",
    "\t\t\t\tfor five in range(0, (max_score-score) // FIVE +1):\n",
    "\t\t\t\t\tscore = minScore(ace, two, three, four, five)\n",
    "\t\t\t\t\tfor six in range(0, (max_score-score) // SIX +1):\n",
    "\t\t\t\t\t\tscore = minScore(ace, two, three, four, five, six)\n",
    "\t\t\t\t\t\tfor seven in range(0, (max_score-score) // SEVEN +1):\n",
    "\t\t\t\t\t\t\tscore = minScore(ace, two, three, four, five, six, seven)\n",
    "\t\t\t\t\t\t\tfor eight in range(0, (max_score-score) // EIGHT +1):\n",
    "\t\t\t\t\t\t\t\tscore = minScore(ace, two, three, four, five, six, seven, eight)\n",
    "\t\t\t\t\t\t\t\tfor nine in range(0, (max_score-score) // NINE +1):\n",
    "\t\t\t\t\t\t\t\t\tscore = minScore(ace, two, three, four, five, six, seven, eight, nine)\n",
    "\t\t\t\t\t\t\t\t\tfor ten in range(0, (max_score-score) // TEN +1):\n",
    "\t\t\t\t\t\t\t\t\t\tscore = minScore(ace, two, three, four, five, six, seven, eight, nine, ten)\n",
    "\t\t\t\t\t\t\t\t\t\tfor jack in range(0, (max_score-score) // JACK +1):\n",
    "\t\t\t\t\t\t\t\t\t\t\tscore = minScore(ace, two, three, four, five, six, seven, eight, nine, ten, jack)\n",
    "\t\t\t\t\t\t\t\t\t\t\tfor queen in range(0, (max_score-score) // QUEEN +1):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tscore = minScore(ace, two, three, four, five, six, seven, eight, nine, ten, jack, queen)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tfor king in range(0, (max_score-score) // KING +1):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tscore = minScore(ace, two, three, four, five, six, seven, eight, nine, ten, jack, queen, king)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tif score <= max_score:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tvalid_hands_to_index[(ace, two, three, four, five, six, seven, eight, nine, ten, jack, queen, king)] = idx\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tidx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre approche est la suivante:\n",
    "+ Créer un réseau de neuronnes capable de jouer efficacement en ayant connaissances de l'état du deck\n",
    "+ Créer un réseau de neuronnes capable d'évaluer la mise optimale à jouer en ayant connaissance de l'état du deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour accélérer le processus d'apprentissages, nous avons besoin de faire jouer plusieurs parties simultanément.  \n",
    "On va donc mettre au point des fonctions agissant sur des matrices dont chaque ligne simule une partie différente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawCard(deck: Deck) -> Card:\n",
    "\t\"\"\"\n",
    "\tDraw a card from the deck and return it\n",
    "\t\"\"\"\n",
    "\tcard = torch.multinomial(deck, 1).item()\n",
    "\tdeck[card] -= 1\n",
    "\treturn card\n",
    "\n",
    "def drawCards(decks: Decks, indices) -> torch.Tensor:\n",
    "\t\"\"\"\n",
    "\tThis function assumes `decks` is a tensor of shape (N, 13), where N is the number of decks.\n",
    "\t`indices` is a tensor of shape (N,) containing the indices of the decks to draw a card from.\n",
    "\t\"\"\"\n",
    "\tcards = torch.multinomial(decks[indices], num_samples=1).squeeze(1)\n",
    "\tdecks[indices, cards] -= 1\n",
    "\treturn cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomDeck() -> Deck:\n",
    "\t\"\"\"\n",
    "\tGenerate a random deck of cards\n",
    "\tThe deck is at first full of cards, then a random number of cards are drawn from it\n",
    "\tThe deck has at least the half of the cards\n",
    "\t\"\"\"\n",
    "\tdeck = torch.tensor([4 * N] * num_cards, dtype=torch.float32)\n",
    "\tnum_cards_to_draw = random.randint(1, num_cards * N * 2)\n",
    "\tfor _ in range(num_cards_to_draw): drawCard(deck)\n",
    "\treturn deck\n",
    "\n",
    "def generateRandomDecks(num_decks: int) -> Decks:\n",
    "\t\"\"\"\n",
    "\tGenerate `num_decks` random decks of cards\n",
    "\t\"\"\"\t\n",
    "\t#return torch.stack([torch.tensor([16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]) for _ in range(num_decks)])\n",
    "\treturn torch.stack([generateRandomDeck() for _ in range(num_decks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomInitialHand() -> Hand:\n",
    "\t\"\"\"\n",
    "\tGenerate a random initial hands for the player and the dealer\n",
    "\t\"\"\"\n",
    "\tdealer_card = random.choice(cards)\n",
    "\tcard1 = random.choice(cards)\n",
    "\tcard2 = random.choice(cards)\n",
    "\thand = F.one_hot(torch.tensor([card1, card2]), num_classes=num_cards).sum(dim=0)\n",
    "\treturn torch.tensor([dealer_card, *hand], dtype=torch.float32)\n",
    "\n",
    "def generateRandomInitialHands(num_games: int) -> Hands:\n",
    "\t\"\"\"\n",
    "\tGenerate `num_games` random initial hands for the player and the dealer\n",
    "\t\"\"\"\n",
    "\treturn torch.stack([generateRandomInitialHand() for _ in range(num_games)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomInitialGames(num_games: int) -> torch.Tensor:\n",
    "\t\"\"\"\n",
    "\tGenerate `num_games` random initial games\n",
    "\tA game is a concatenation of a deck and two initial hands\n",
    "\t\"\"\"\n",
    "\treturn torch.cat([generateRandomDecks(num_games), generateRandomInitialHands(num_games)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handsScoreMin(hands: Hands) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tScore minimal of a hand\n",
    "\tEach ace is counted as 1 if the total score is less than or equal to 21, otherwise it is counted as 1\n",
    "\t\"\"\"\t\n",
    "\treturn torch.sum(hands * torch.tensor(card_values, dtype=torch.float32), dim=1)\n",
    "\n",
    "def handsScore(hands: Hands) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tScore of a hand\n",
    "\tEach ace is counted as 11 if the total score is less than or equal to the max_score (=21), otherwise it is counted as 1\n",
    "\t\"\"\"\t\n",
    "\tscore = handsScoreMin(hands)\n",
    "\tcan_use_ace_index = score + 10 <= max_score\n",
    "\thave_ace_index = hands[:, 0] > 0\n",
    "\tscore[torch.logical_and(can_use_ace_index, have_ace_index)] += 10\n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamesFinished(player_hands: Hands, actions: Actions) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tCheck if the game is finished\n",
    "\tA game is finished if the player is standing or has a score greater than 21\n",
    "\t\"\"\"\n",
    "\treturn torch.logical_or((actions == STAND), (handsScoreMin(player_hands) > max_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi implémenter les politiques classiques des dealers sous forme multi-games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standOn17Hard(dealer_hands: Hands, player_hands: Hands) -> Actions:\n",
    "\t\"\"\"\n",
    "\tDealer stand on 17 hard\n",
    "\t\"\"\"\n",
    "\treturn torch.where(handsScore(dealer_hands) >= 17, torch.tensor(STAND), torch.tensor(HIT))\n",
    "\n",
    "def standOn17Soft(dealer_hands: Hands, player_hands: Hands) -> Actions:\n",
    "\t\"\"\"\n",
    "\tDealer stand on 17 soft\n",
    "\t\"\"\"\n",
    "\treturn torch.where(handsScoreMin(dealer_hands) >= 17, torch.tensor(STAND), torch.tensor(HIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dealer_policy: Policy = standOn17Soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit aussi adpater les fonctions qui déterminent le résultat d'une partie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBlackjacks(hands: Hands, scores: torch.Tensor) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tCheck if a hand is a blackjack\n",
    "\tScore is the score of the hand if it is already computed\n",
    "\t\"\"\"\n",
    "\treturn torch.logical_and(torch.eq(scores, max_score), torch.eq(torch.sum(hands, dim=1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handsComparisonPlayerPOV(dealer_hands: Hands, player_hands: Hands) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tCompare two valid hands\n",
    "\tReturn 1 if the dealer wins, 0 if it is a draw and -1 if the player wins\n",
    "\tCheck if there is a blackjack for the dealer and the player\n",
    " \n",
    "\tThe order of the verification is really important\n",
    "\t\"\"\"\n",
    "\tresult = torch.zeros(dealer_hands.shape[0])\n",
    "\tscore_dealers = handsScore(dealer_hands)\n",
    "\tscore_players = handsScore(player_hands)\n",
    "\tdealer_blackjacks = checkBlackjacks(dealer_hands, score_dealers)\n",
    "\tplayer_blackjacks = checkBlackjacks(player_hands, score_players)\n",
    "\tresult[score_dealers > score_players] = -1\n",
    "\tresult[score_dealers < score_players] = 1\n",
    "\tresult[score_dealers > max_score] = 1\n",
    "\tresult[dealer_blackjacks] = -1\n",
    "\tresult[player_blackjacks] = 1.5\n",
    "\tresult[torch.logical_and(dealer_blackjacks, player_blackjacks)] = 0\n",
    "\tresult[score_players > max_score] = -1\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishGames(dealer_hands: Hands, player_hands: Hands, decks: Decks) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tResolve every game\n",
    "\tReturn the reward for the player\n",
    "\t\"\"\"\n",
    "\tneed_actions = torch.tensor([True] * dealer_hands.shape[0])\n",
    "\twhile torch.any(need_actions):\n",
    "\t\tactions = dealer_policy(dealer_hands, player_hands)\n",
    "\t\thit_indices = torch.where(actions == HIT)[0]\n",
    "\t\tdealer_hands.index_add_(0, hit_indices, F.one_hot(drawCards(decks, hit_indices), num_classes=13).to(torch.float32))\n",
    "\t\tneed_actions = torch.logical_not(gamesFinished(player_hands, actions))\n",
    "\treturn handsComparisonPlayerPOV(dealer_hands, player_hands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut enfin simuler un ensemble de parties simultanément"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simulateGames(hands_and_decks: torch.Tensor, model_play):\n",
    "\t\"\"\"\n",
    "\tSimulate games with the given initial hands and decks\n",
    "\tReturn the gain of the player\n",
    "\t\"\"\"\n",
    "\tnum_games: int = hands_and_decks.shape[0]\n",
    "\tdecks = hands_and_decks[:, :13]\n",
    "\tdealer_cards = hands_and_decks[:, 13]\n",
    "\tplayer_hands = hands_and_decks[:, 14:]\n",
    "\tneed_actions = torch.tensor([True] * num_games)\n",
    "\twhile torch.any(need_actions):\n",
    "\t\tactions = torch.tensor([STAND] * num_games)\n",
    "\t\tactions[need_actions] = torch.argmax(model_play(hands_and_decks[need_actions]), dim=1)\n",
    "\t\thit_indices = torch.where(actions == HIT)[0]\n",
    "\t\tplayer_hands.index_add_(0, hit_indices, F.one_hot(drawCards(decks, hit_indices), num_classes=13).to(torch.float32))\n",
    "\t\tneed_actions = torch.logical_not(gamesFinished(player_hands, actions))\n",
    "\tdealer_hands = F.one_hot(dealer_cards.to(torch.int64), num_classes=num_cards).to(torch.float32)\n",
    "\treturn finishGames(dealer_hands, player_hands, decks)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simulateGameRepeat(hands_and_deck: torch.Tensor, model_play):\n",
    "\t\"\"\"\n",
    "\tSimulate a game\n",
    "\tReturn the gain of the player\n",
    "\t\"\"\"\n",
    "\tmean_score = torch.zeros(hands_and_deck.shape[0])\n",
    "\tfor _ in range(nb_simulation):\n",
    "\t\tinital_hand_and_deck_copy = hands_and_deck.clone()\n",
    "\t\tmean_score += simulateGames(inital_hand_and_deck_copy, model_play) / nb_simulation\n",
    "\treturn mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simulateGameAfterFirstAction(hands_and_deck:torch.Tensor, model_play, actions: torch.Tensor):\n",
    "\t\"\"\"\n",
    "\tSimulate a game\n",
    "\tReturn the gain of the player\n",
    "\t\"\"\"\n",
    "\tnum_games = hands_and_deck.shape[0]\n",
    "\tdecks = hands_and_deck[:, :13]\n",
    "\tdealer_cards = hands_and_deck[:, 13]\n",
    "\tplayer_hands = hands_and_deck[:, 14:]\n",
    "\tneed_actions = torch.logical_not(gamesFinished(player_hands, actions))\n",
    "\twhile torch.any(need_actions):\n",
    "\t\thit_indices = torch.where(actions == HIT)[0]\n",
    "\t\tplayer_hands.index_add_(0, hit_indices, F.one_hot(drawCards(decks, hit_indices), num_classes=num_cards).to(torch.float32))\n",
    "\t\tactions[need_actions] = torch.argmax(model_play(hands_and_deck[need_actions]), dim=1)\n",
    "\t\tactions[torch.logical_not(need_actions)] = STAND\n",
    "\t\tneed_actions = torch.logical_not(gamesFinished(player_hands, actions))\n",
    "\tdealer_hands = F.one_hot(dealer_cards.to(torch.int64), num_classes=num_cards).to(torch.float32)\n",
    "\t\n",
    "\tmean_score = torch.zeros(num_games)\n",
    "\tfor _ in range(nb_simulation):\n",
    "\t\tdealer_hands_copy = dealer_hands.clone()\n",
    "\t\tplayer_hands_copy = player_hands.clone()\n",
    "\t\tdecks_copy = decks.clone()\n",
    "\t\tmean_score += finishGames(dealer_hands_copy, player_hands_copy, decks_copy) * ratio_mean\n",
    "\treturn mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simulateGameAfterFirstActionRepeat(hands_and_deck: torch.Tensor, model_play, actions:torch.Tensor):\n",
    "\t\"\"\"\n",
    "\tSimulate a game\n",
    "\tReturn the gain of the player\n",
    "\t\"\"\"\n",
    "\tmean_score = torch.zeros(hands_and_deck.shape[0])\n",
    "\tfor _ in range(nb_simulation):\n",
    "\t\tinital_hand_and_deck_copy = hands_and_deck.clone()\n",
    "\t\tmean_score += simulateGameAfterFirstAction(inital_hand_and_deck_copy, model_play, actions) * ratio_mean\n",
    "\treturn mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant paramétrer l'architecture choisie et créer des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_model_hidden_layers = [13]\n",
    "play_model_hidden_layers = [27]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBetModel() -> torch.nn.Module:\n",
    "\t\"\"\"\n",
    "\tCreate the bet model\n",
    "\t\"\"\"\n",
    "\tlayers = [13, *bet_model_hidden_layers, 1]\n",
    "\tsequence = []\n",
    "\tfor i in range(len(layers)-1):\n",
    "\t\tsequence.append(torch.nn.Linear(layers[i], layers[i+1], dtype=torch.float32))\n",
    "\t\tsequence.append(torch.nn.ReLU())\n",
    "\treturn torch.nn.Sequential(*sequence)\n",
    "\n",
    "def makePlayModel() -> torch.nn.Module:\n",
    "\t\"\"\"\n",
    "\tCreate the play model\n",
    "\t\"\"\"\n",
    "\tlayers = [27, *play_model_hidden_layers, 2]\n",
    "\tsequence = []\n",
    "\tfor i in range(len(layers)-1):\n",
    "\t\tsequence.append(torch.nn.Linear(layers[i], layers[i+1], dtype=torch.float32))\n",
    "\t\tsequence.append(torch.nn.ReLU())\n",
    "\tsequence.pop()\n",
    "\tsequence.append(torch.nn.Tanh())\n",
    "\treturn torch.nn.Sequential(*sequence)\n",
    "\n",
    "def makePlayModelPrior():\n",
    "\t\"\"\"\n",
    "\tCreate the play model\n",
    "\t\"\"\"\n",
    "\tlayers = [27, *play_model_hidden_layers, 2]\n",
    "\tsequence = []\n",
    "\tfor i in range(len(layers)-1):\n",
    "\t\tsequence.append(torch.nn.Linear(layers[i], layers[i+1], dtype=torch.float32))\n",
    "\t\tsequence.append(torch.nn.ReLU())\n",
    "\tsequence.pop()\n",
    "\tsequence.append(torch.nn.Softmax(dim=1))\n",
    "\treturn torch.nn.Sequential(*sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelBaseline(hands_and_decks: torch.Tensor) -> torch.Tensor:\n",
    "\t\"\"\"\n",
    "\tCompute the model with the matrix of Q1\n",
    "\t\"\"\"\n",
    "\tgoal_gains_matrix = torch.tensor(np.load(\"gain.npy\"), dtype=torch.float32)\n",
    "\tdealer_cards = hands_and_decks[:, 13]\n",
    "\tplayer_hands = hands_and_decks[:, 14:]\n",
    "\tgoal_gains = torch.zeros((hands_and_decks.shape[0], 2))\n",
    "\tfor i, (dealer_card, player_hands) in enumerate(zip(dealer_cards, player_hands)):\n",
    "\t\tplayer_hands = tuple(player_hands.to(torch.int64).tolist())\n",
    "\t\tdealer_card = dealer_card.to(torch.int64).item()\n",
    "\t\tgoal_gain = goal_gains_matrix[valid_hands_to_index[player_hands], dealer_card]\n",
    "\t\tgoal_gains[i] = goal_gain\n",
    "\treturn goal_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateGameWithBet(hands_and_decks: torch.Tensor, model_play, model_bet):\n",
    "\t\"\"\"\n",
    "\tSimulate a game\n",
    "\tReturn the gain of the player\n",
    "\t\"\"\"\n",
    "\tdecks = hands_and_decks[:, :13]\n",
    "\tdealer_cards = hands_and_decks[:, 13]\n",
    "\tplayer_hands = hands_and_decks[:, 14:]\n",
    "\tneed_actions = torch.tensor([True] * hands_and_decks.shape[0])\n",
    "\twhile torch.any(need_actions):\n",
    "\t\tactions = torch.tensor([STAND] * hands_and_decks.shape[0])\n",
    "\t\tactions[need_actions] = torch.argmax(model_play(hands_and_decks[need_actions]), dim=1)\n",
    "\t\thit_indices = torch.where(actions == HIT)[0]\n",
    "\t\tplayer_hands.index_add_(0, hit_indices, F.one_hot(drawCards(decks, hit_indices), num_classes=13).to(torch.float32))\n",
    "\t\tneed_actions = torch.logical_not(gamesFinished(player_hands, actions))\n",
    "\tdealer_hands = F.one_hot(dealer_cards.to(torch.int64), num_classes=num_cards).to(torch.float32)\n",
    "\tmean_score = torch.zeros(hands_and_decks.shape[0])\n",
    "\tbet = model_bet(decks).squeeze(1)\n",
    "\tfor i in range(nb_simulation):\n",
    "\t\tdealer_hands_copy = dealer_hands.clone()\n",
    "\t\tplayer_hands_copy = player_hands.clone()\n",
    "\t\tdecks_copy = decks.clone()\n",
    "\t\tmean_score += finishGames(dealer_hands_copy, player_hands_copy, decks_copy) / nb_simulation * bet\n",
    "\treturn mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateGameWithBetRepeat(hands_and_decks: torch.Tensor, model_play, model_bet):\n",
    "\t\"\"\"\n",
    "\tSimulate a game\n",
    "\tReturn the gain of the player\n",
    "\t\"\"\"\n",
    "\tmean_score = torch.zeros(hands_and_decks.shape[0])\n",
    "\tfor _ in range(nb_simulation):\n",
    "\t\tinital_hand_and_deck_copy = hands_and_decks.clone()\n",
    "\t\tmean_score += simulateGameWithBet(inital_hand_and_deck_copy, model_play, model_bet) / nb_simulation\n",
    "\treturn mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evalModel(model_baseline, model_play, batch_size: int):\n",
    "\t\"\"\"\n",
    "\tEvaluate the model\n",
    "\t\"\"\"\n",
    "\tmodel_play.eval()\n",
    "\thands_and_decks = generateRandomInitialGames(batch_size)\n",
    "\thands_and_decks_cp = hands_and_decks.clone()\n",
    "\tgains = simulateGameRepeat(hands_and_decks, model_play)\n",
    "\taverage_gain = torch.mean(gains)\n",
    "\tgains_baseline = simulateGameRepeat(hands_and_decks_cp, model_baseline)\n",
    "\taverage_gain_baseline = torch.mean(gains_baseline)\n",
    "\treturn average_gain, average_gain_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainPlayModel(model_baseline, model_play, optimizer_play, batch_size: int, num_epochs: int):\n",
    "\tmodel_play\n",
    "\tfor epoch in range(num_epochs + 1):\n",
    "\t\tmodel_play.train()\n",
    "\t\thands_and_decks = generateRandomInitialGames(batch_size)\n",
    "\t\thands_and_decks_cp1 = hands_and_decks.clone()\n",
    "\t\thands_and_decks_cp2 = hands_and_decks.clone()\n",
    "\t\t\n",
    "\t\tgoal_gains = torch.zeros((batch_size, 2))\n",
    "\t\tactions_hit = torch.tensor([HIT] * hands_and_decks.shape[0])\n",
    "\t\tgoal_gains[:, 0] = simulateGameAfterFirstActionRepeat(hands_and_decks_cp1, model_play, actions_hit)\n",
    "\t\tactions_stand = torch.tensor([STAND] * hands_and_decks.shape[0])\n",
    "\t\tgoal_gains[:, 1] = simulateGameAfterFirstActionRepeat(hands_and_decks_cp2, model_play, actions_stand)\n",
    "\t\toptimizer_play.zero_grad()\n",
    "\t\tactions = model_play(hands_and_decks)\n",
    "\t\tloss = F.mse_loss(actions, goal_gains)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer_play.step()\n",
    "\t\tif epoch % 100 == 0:\n",
    "\t\t\taverage_gain, average_gain_baseline = evalModel(model_baseline, model_play, batch_size)\n",
    "\t\t\tprint(f\"epoch: {epoch}, loss: {loss.item()}, average gain: {average_gain}, average gain baseline: {average_gain_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainPlayModelPrior(model_baseline, model_play, optimizer_play, batch_size: int, num_epochs: int):\n",
    "\tgoal_gains_matrix = torch.tensor(np.load(\"gain.npy\"), dtype=torch.float32)\n",
    "\tfor epoch in range(num_epochs + 1):\n",
    "\t\thands_and_decks = generateRandomInitialGames(batch_size)\n",
    "\t\tdealer_cards = hands_and_decks[:, 13]\n",
    "\t\tplayer_hands = hands_and_decks[:, 14:]\n",
    "\t\tgoal_gains = torch.zeros((batch_size, 2))\n",
    "\n",
    "\t\tfor i, (dealer_card, player_hands) in enumerate(zip(dealer_cards, player_hands)):\n",
    "\t\t\tplayer_hands = tuple(player_hands.to(torch.int64).tolist())\n",
    "\t\t\tdealer_card = dealer_card.to(torch.int64).item()\n",
    "\t\t\tgoal_gain = torch.softmax(goal_gains_matrix[valid_hands_to_index[player_hands], dealer_card], dim=0)\n",
    "\t\t\tgoal_gains[i] = goal_gain\n",
    "\t\toptimizer_play.zero_grad()\n",
    "\t\tactions = model_play(hands_and_decks)\n",
    "\t\tloss = F.cross_entropy(actions, goal_gains)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer_play.step()\n",
    "\t\tif epoch % 100 == 0:\n",
    "\t\t\taverage_gain, average_gain_baseline = evalModel(model_baseline, model_play, batch_size)\n",
    "\t\t\tprint(f\"epoch: {epoch}, loss: {loss.item()}, average gain: {average_gain}, average gain baseline: {average_gain_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBetModel(model_play, model_bet,  optimizer_bet, batch_size: int, hand_played_per_deck: int, num_epochs: int):\n",
    "\tmodel_play.eval()\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tinitial_decks = generateRandomDecks(batch_size)\n",
    "\t\tgain_deck = torch.zeros((batch_size, hand_played_per_deck))\n",
    "\t\tfor i in range(hand_played_per_deck):\n",
    "\t\t\thands = generateRandomInitialHands(batch_size)\n",
    "\t\t\thands_and_decks = torch.cat([initial_decks, hands], dim=1)\n",
    "\t\t\thands_and_decks_cp1 = hands_and_decks.clone()\n",
    "\t\t\thands_and_decks_cp2 = hands_and_decks.clone()\n",
    "\t\t\thands_and_decks_cp3 = hands_and_decks.clone()\n",
    "\t\t\tgoal_gains = torch.zeros((batch_size, 2))\n",
    "\t\t\tactions_hit = torch.tensor([HIT] * hands_and_decks.shape[0])\n",
    "\t\t\tgoal_gains[:, 0] = simulateGameAfterFirstActionRepeat(hands_and_decks_cp1, model_play, actions_hit)\n",
    "\t\t\tactions_stand = torch.tensor([STAND] * hands_and_decks.shape[0])\n",
    "\t\t\tgoal_gains[:, 1] = simulateGameAfterFirstActionRepeat(hands_and_decks_cp2, model_play, actions_stand)\n",
    "\t\t\tgoal_gains = torch.max(goal_gains, 1).values\n",
    "\t\t\tgain_deck[:, i] = goal_gains\n",
    "\t\tgain_deck = (torch.max(torch.tensor(-0.1), gain_deck.mean(dim=1)) + 0.1) * 100\n",
    "\t\tbet = model_bet(initial_decks).squeeze(1)\n",
    "\t\toptimizer_bet.zero_grad()\n",
    "\t\tloss = F.mse_loss(bet, goal_gains)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer_bet.step()\n",
    "\t\tif epoch % 100 == 0:\n",
    "\t\t\tgains = simulateGameWithBetRepeat(hands_and_decks_cp3, model_play, model_bet)\n",
    "\t\t\taverage_gain = torch.mean(gains)\n",
    "\t\t\tprint(f\"epoch: {epoch}, loss: {loss.item()}, average gain: {average_gain}\")\n",
    "\tgains = simulateGameWithBetRepeat(hands_and_decks_cp3, model_play, model_bet)\n",
    "\taverage_gain = torch.mean(gains)\n",
    "\tprint(f\"epoch: {epoch}, loss: {loss.item()}, average gain: {average_gain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_epochs = 1000\n",
    "hand_played_per_deck = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est maintenant en mesure de faire jouer un modèle (non entraîné)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_play = makePlayModelPrior()\n",
    "optimizer_play = torch.optim.Adam(model_play.parameters(), lr=0.0001)\n",
    "trainPlayModelPrior(modelBaseline, model_play, optimizer_play, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_play = makePlayModel()\n",
    "optimizer_play = torch.optim.Adam(model_play.parameters(), lr=0.0001)\n",
    "trainPlayModel(modelBaseline, model_play, optimizer_play, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bet = makeBetModel()\n",
    "optimizer_bet = torch.optim.Adam(model_bet.parameters(), lr=0.0001)\n",
    "trainBetModel(model_play, model_bet, optimizer_bet, batch_size, hand_played_per_deck, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
