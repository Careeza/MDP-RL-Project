{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Card = int\n",
    "Action = int\n",
    "\n",
    "ACE: Card = 1\n",
    "TWO: Card = 2\n",
    "THREE: Card = 3\n",
    "FOUR: Card = 4\n",
    "FIVE: Card = 5\n",
    "SIX: Card = 6\n",
    "SEVEN: Card = 7\n",
    "EIGHT: Card = 8\n",
    "NINE: Card = 9\n",
    "TEN: Card = 10\n",
    "JACK: Card = 10\n",
    "QUEEN: Card = 10\n",
    "KING: Card = 10\n",
    "\n",
    "card_values: list[Card] = [ACE, TWO, THREE, FOUR, FIVE, SIX, SEVEN, EIGHT, NINE, TEN, JACK, QUEEN, KING]\n",
    "num_cards: int = len(card_values)\n",
    "cards: list[int] = list(range(num_cards))\n",
    "\n",
    "max_card_value: int = max(card_values)\n",
    "max_score: int = 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HIT: Action = 0 \n",
    "STAND: Action = 1\n",
    "DOUBLE: Action = 2\n",
    "SPLIT: Action = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 7\n",
    "max_card_per_hand = 28\n",
    "hand_played_per_simulation = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_score_min(ace, two=0, three=0, four=0, five=0, six=0, seven=0, eight=0, nine=0, ten=0, jack=0, queen=0, king=0):\n",
    "\tscore = 0\n",
    "\tscore += ace\n",
    "\tscore += 2*two\n",
    "\tscore += 3*three\n",
    "\tscore += 4*four\n",
    "\tscore += 5*five\n",
    "\tscore += 6*six\n",
    "\tscore += 7*seven\n",
    "\tscore += 8*eight\n",
    "\tscore += 9*nine\n",
    "\tscore += 10*ten\n",
    "\tscore += 10*jack\n",
    "\tscore += 10*queen\n",
    "\tscore += 10*king\n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_hands_to_index = {}\n",
    "index = 0\n",
    "for ace in range(0, 22):\n",
    "\tscore = count_score_min(ace)\n",
    "\tfor two in range(0, (21 - score) // 2 + 1):\n",
    "\t\tscore = count_score_min(ace, two)\n",
    "\t\tfor three in range(0, (21 - score) // 3 + 1):\n",
    "\t\t\tscore = count_score_min(ace, two, three)\n",
    "\t\t\tfor four in range(0, (21 - score) // 4 + 1):\n",
    "\t\t\t\tscore = count_score_min(ace, two, three, four)\n",
    "\t\t\t\tfor five in range(0, (21 - score) // 5 + 1):\n",
    "\t\t\t\t\tscore = count_score_min(ace, two, three, four, five)\n",
    "\t\t\t\t\tfor six in range(0, (21 - score) // 6 + 1):\n",
    "\t\t\t\t\t\tscore = count_score_min(ace, two, three, four, five, six)\n",
    "\t\t\t\t\t\tfor seven in range(0, (21 - score) // 7 + 1):\n",
    "\t\t\t\t\t\t\tscore = count_score_min(ace, two, three, four, five, six, seven)\n",
    "\t\t\t\t\t\t\tfor eight in range(0, (21 - score) // 8 + 1):\n",
    "\t\t\t\t\t\t\t\tscore = count_score_min(ace, two, three, four, five, six, seven, eight)\n",
    "\t\t\t\t\t\t\t\tfor nine in range(0, (21 - score) // 9 + 1):\n",
    "\t\t\t\t\t\t\t\t\tscore = count_score_min(ace, two, three, four, five, six, seven, eight, nine)\n",
    "\t\t\t\t\t\t\t\t\tfor ten in range(0, (21 - score) // 10 + 1):\n",
    "\t\t\t\t\t\t\t\t\t\tscore = count_score_min(ace, two, three, four, five, six, seven, eight, nine, ten)\n",
    "\t\t\t\t\t\t\t\t\t\tfor jack in range(0, (21 - score) // 10 + 1):\n",
    "\t\t\t\t\t\t\t\t\t\t\tscore = count_score_min(ace, two, three, four, five, six, seven, eight, nine, ten, jack)\n",
    "\t\t\t\t\t\t\t\t\t\t\tfor queen in range(0, (21 - score) // 10 + 1):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tscore = count_score_min(ace, two, three, four, five, six, seven, eight, nine, ten, jack, queen)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tfor king in range(0, (21 - score) // 10 + 1):\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tscore = count_score_min(ace, two, three, four, five, six, seven, eight, nine, ten, jack, queen, king)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tif score <= 21:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tvalid_hands_to_index[(ace, two, three, four, five, six, seven, eight, nine, ten, jack, queen, king)] = index\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tindex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawCard(deck):\n",
    "\t\"\"\"\n",
    "\tDraw a card from the deck and return it\n",
    "\t\"\"\"\n",
    "\tcard = torch.multinomial(deck, 1).item()\n",
    "\tdeck[card] -= 1\n",
    "\treturn card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawCards(decks, indices):\n",
    "\t\"\"\"\n",
    "\tThis function assumes `deck` is a tensor of shape (N, 13), where N is the number of decks.\n",
    "\t\"\"\"\n",
    "\t# Calculate probabilities for each deck and draw cards\n",
    "\tcards = torch.multinomial(decks[indices], num_samples=1).squeeze(1)\n",
    "\tdecks[indices, cards] -= 1\n",
    "\treturn cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomDeck() -> torch.tensor:\n",
    "\tdeck = torch.tensor([4 * N] * num_cards, dtype=torch.float32)\n",
    "\tnum_cards_to_draw = random.randint(1, num_cards * N * 2)\n",
    "\tfor _ in range(num_cards_to_draw):\n",
    "\t\tdrawCard(deck)\n",
    "\treturn deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomDecks(n: int) -> torch.tensor:\n",
    "\treturn torch.stack([torch.tensor([16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]) for _ in range(n)])\n",
    "\t# return torch.stack([generateRandomDeck() for _ in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomInitialHand():\n",
    "\tdealer_card = random.choice(cards)\n",
    "\tcard1 = random.choice(cards)\n",
    "\tcard2 = random.choice(cards)\n",
    "\thand = F.one_hot(torch.tensor([card1, card2]), num_classes=num_cards).sum(dim=0)\n",
    "\treturn torch.tensor([dealer_card, *hand], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomInitialHands(n: int) -> torch.tensor:\n",
    "\treturn torch.stack([generateRandomInitialHand() for _ in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomInitialHandAndDeck(n: int) -> torch.tensor:\n",
    "\treturn torch.cat([generateRandomDecks(n), generateRandomInitialHands(n)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 27])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateRandomInitialHandAndDeck(128).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handsScoreMin(hands: torch.tensor) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tScore of a hand\n",
    "\tEach ace is counted as 11 if the total score is less than or equal to 21, otherwise it is counted as 1\n",
    "\t\"\"\"\t\n",
    "\treturn torch.sum(hands * torch.tensor(card_values, dtype=torch.float32), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handsScore(hands: torch.tensor) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tScore of a hand\n",
    "\tEach ace is counted as 11 if the total score is less than or equal to 21, otherwise it is counted as 1\n",
    "\t\"\"\"\t\n",
    "\tscore = handsScoreMin(hands)\n",
    "\tcan_use_ace_index = score + 10 <= max_score\n",
    "\thave_ace_index = hands[:, 0] > 0\n",
    "\tscore[torch.logical_and(can_use_ace_index, have_ace_index)] += 10\n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamesFinished(player_hands, actions) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tCheck if the game is finished\n",
    "\t\"\"\"\n",
    "\treturn torch.logical_or((actions == STAND), (handsScoreMin(player_hands) > max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stand_on_17_hard(dealer_hands, player_hands):\n",
    "\t\"\"\"\n",
    "\tDealer stand on 17 hard\n",
    "\t\"\"\"\n",
    "\tscore = handsScore(dealer_hands)\n",
    "\treturn torch.where(score >= 17, torch.tensor(STAND), torch.tensor(HIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBlackjacks(hands, scores) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tCheck if a hand is a blackjack\n",
    "\tScore is the score of the hand if it is already computed\n",
    "\t\"\"\"\n",
    "\treturn torch.logical_and(torch.eq(scores, max_score), torch.eq(torch.sum(hands, dim=1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handsComparisonPlayerPOV(dealer_hands, player_hands) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tCompare two valid hands\n",
    "\tReturn 1 if the dealer wins, 0 if it is a draw and -1 if the player wins\n",
    "\tCheck if there is a blackjack for the dealer and the player\n",
    "\t\"\"\"\n",
    "\tresult = torch.zeros(dealer_hands.shape[0])\n",
    "\tscore_dealers = handsScore(dealer_hands)\n",
    "\tscore_players = handsScore(player_hands)\n",
    "\tdealer_blackjack = checkBlackjacks(dealer_hands, score_dealers)\n",
    "\tplayer_blackjack = checkBlackjacks(player_hands, score_players)\n",
    "\tresult[score_dealers > score_players] = -1\n",
    "\tresult[score_dealers < score_players] = 1\n",
    "\tresult[score_dealers > max_score] = 1\n",
    "\tresult[dealer_blackjack] = -1\n",
    "\tresult[player_blackjack] = 1.5\n",
    "\tresult[torch.logical_and(dealer_blackjack, player_blackjack)] = 0\n",
    "\tresult[score_players > max_score] = -1\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolveGames(dealer_hands, player_hands, decks) -> torch.tensor:\n",
    "\t\"\"\"\n",
    "\tResolve a game\n",
    "\tReturn the reward for the player\n",
    "\t\"\"\"\n",
    "\tneed_actions = torch.tensor([True] * dealer_hands.shape[0])\n",
    "\twhile torch.any(need_actions):\n",
    "\t\tactions = stand_on_17_hard(dealer_hands, player_hands)\n",
    "\t\thit_indices = torch.where(actions == HIT)[0]\n",
    "\t\tdealer_hands.index_add_(0, hit_indices, F.one_hot(drawCards(decks, hit_indices), num_classes=13).to(torch.float32))\n",
    "\t\tneed_actions = torch.logical_not(gamesFinished(player_hands, actions))\n",
    "\treturn handsComparisonPlayerPOV(dealer_hands, player_hands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simulateGame(inital_hand_and_deck, model_play):\n",
    "\t\"\"\"\n",
    "\tSimulate a game\n",
    "\tReturn the gain of the player\n",
    "\t\"\"\"\n",
    "\tdecks = inital_hand_and_deck[:, :13]\n",
    "\tdealer_cards = inital_hand_and_deck[:, 13]\n",
    "\tplayer_hands = inital_hand_and_deck[:, 14:]\n",
    "\tneed_actions = torch.tensor([True] * inital_hand_and_deck.shape[0])\n",
    "\twhile torch.any(need_actions):\n",
    "\t\tactions = torch.tensor([STAND] * inital_hand_and_deck.shape[0])\n",
    "\t\tactions[need_actions] = torch.argmax(model_play(inital_hand_and_deck[need_actions]), dim=1)\n",
    "\t\thit_indices = torch.where(actions == HIT)[0]\n",
    "\t\tplayer_hands.index_add_(0, hit_indices, F.one_hot(drawCards(decks, hit_indices), num_classes=13).to(torch.float32))\n",
    "\t\tneed_actions = torch.logical_not(gamesFinished(player_hands, actions))\n",
    "\tdealer_hands = F.one_hot(dealer_cards.to(torch.int64), num_classes=num_cards).to(torch.float32)\n",
    "\treturn resolveGames(dealer_hands, player_hands, decks)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bet_model():\n",
    "\treturn torch.nn.Sequential(\n",
    "\t\ttorch.nn.Linear(13, 13),\n",
    "\t\ttorch.nn.ReLU(),\n",
    "\t\ttorch.nn.Linear(13, 1),\n",
    "\t\ttorch.nn.ReLU()\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_play_model():\n",
    "\treturn torch.nn.Sequential(\n",
    "\t\ttorch.nn.Linear(27, 27, dtype=torch.float32),\n",
    "\t\ttorch.nn.ReLU(),\n",
    "\t\ttorch.nn.Linear(27, 2, dtype=torch.float32),\n",
    "\t\ttorch.nn.Softmax(dim=1)\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_hand_and_deck = generateRandomInitialHandAndDeck(128)\n",
    "model_play = make_play_model()\n",
    "simulateGame(initial_hand_and_deck, model_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_play_model(model_play, optimizer_play, num_epochs):\n",
    "\t# goal_gain load gain.npy\n",
    "\tgoal_gains_matrix = torch.tensor(np.load(\"gain.npy\"), dtype=torch.float32)\n",
    "\tinitial_hands_and_decks = generateRandomInitialHandAndDeck(batch_size)\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tdecks = initial_hands_and_decks[:, :13]\n",
    "\t\tdealer_cards = initial_hands_and_decks[:, 13]\n",
    "\t\tplayer_hands = initial_hands_and_decks[:, 14:]\n",
    "\t\tgoal_gains = torch.zeros((batch_size, 2))\n",
    "\n",
    "\t\tfor i, (dealer_card, player_hands) in enumerate(zip(dealer_cards, player_hands)):\n",
    "\t\t\tplayer_hands = tuple(player_hands.to(torch.int64).tolist())\n",
    "\t\t\tdealer_card = dealer_card.to(torch.int64).item()\n",
    "\t\t\tgoal_gain = F.one_hot(torch.argmax(goal_gains_matrix[valid_hands_to_index[player_hands], dealer_card]), num_classes=2).to(torch.float32)\n",
    "\t\t\t# goal_gain = torch.softmax(goal_gains_matrix[valid_hands_to_index[player_hands], dealer_card], dim=0)\n",
    "\t\t\tgoal_gains[i] = goal_gain\n",
    "\t\toptimizer_play.zero_grad()\n",
    "\t\tactions = model_play(initial_hands_and_decks)\n",
    "\t\tloss = F.cross_entropy(actions, goal_gains)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer_play.step()\n",
    "\t\tif epoch % 100 == 0:\n",
    "\t\t\tcopy_initial_hands_and_decks = initial_hands_and_decks.clone()\n",
    "\t\t\tgains = simulateGame(copy_initial_hands_and_decks, model_play)\n",
    "\t\t\taverage_gain = torch.mean(gains)\n",
    "\t\t\tprint(f\"epoch: {epoch}, loss: {loss.item()}, average gain: {average_gain}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_play_model(model_play, optimizer_play, num_epochs):\n",
    "\t# goal_gain load gain.npy\n",
    "\tgoal_gains_matrix = torch.tensor(np.load(\"gain.npy\"), dtype=torch.float32)\n",
    "\tinitial_hands_and_decks = generateRandomInitialHandAndDeck(batch_size)\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tdecks = initial_hands_and_decks[:, :13]\n",
    "\t\tdealer_cards = initial_hands_and_decks[:, 13]\n",
    "\t\tplayer_hands = initial_hands_and_decks[:, 14:]\n",
    "\t\tgoal_gains = torch.zeros((batch_size, 2))\n",
    "\n",
    "\t\tfor i, (dealer_card, player_hands) in enumerate(zip(dealer_cards, player_hands)):\n",
    "\t\t\tplayer_hands = tuple(player_hands.to(torch.int64).tolist())\n",
    "\t\t\tdealer_card = dealer_card.to(torch.int64).item()\n",
    "\t\t\tgoal_gain = F.one_hot(torch.argmax(goal_gains_matrix[valid_hands_to_index[player_hands], dealer_card]), num_classes=2).to(torch.float32)\n",
    "\t\t\t# goal_gain = torch.softmax(goal_gains_matrix[valid_hands_to_index[player_hands], dealer_card], dim=0)\n",
    "\t\t\tgoal_gains[i] = goal_gain\n",
    "\t\toptimizer_play.zero_grad()\n",
    "\t\tactions = model_play(initial_hands_and_decks)\n",
    "\t\tloss = F.cross_entropy(actions, goal_gains)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer_play.step()\n",
    "\t\tif epoch % 100 == 0:\n",
    "\t\t\tcopy_initial_hands_and_decks = initial_hands_and_decks.clone()\n",
    "\t\t\tgains = simulateGame(copy_initial_hands_and_decks, model_play)\n",
    "\t\t\taverage_gain = torch.mean(gains)\n",
    "\t\t\tprint(f\"epoch: {epoch}, loss: {loss.item()}, average gain: {average_gain}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_play_model(model_play, optimizer_play, num_epochs):\n",
    "\t# goal_gain load gain.npy\n",
    "\tinitial_hands_and_decks = generateRandomInitialHandAndDeck(batch_size)\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tcopy_initial_hands_and_decks = initial_hands_and_decks.clone()\n",
    "\t\tdecks = initial_hands_and_decks[:, :13]\n",
    "\t\tdealer_cards = initial_hands_and_decks[:, 13]\n",
    "\t\tplayer_hands = initial_hands_and_decks[:, 14:]\n",
    "\n",
    "\t\tgains = simulateGame(copy_initial_hands_and_decks, model_play)\n",
    "\t\taverage_gain = torch.mean(gains)\n",
    "\t\toptimizer_play.zero_grad()\n",
    "\t\tactions = model_play(initial_hands_and_decks)\n",
    "\t\tloss = -average_gain\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer_play.step()\n",
    "\t\tif epoch % 100 == 0:\n",
    "\t\t\tgains = simulateGame(copy_initial_hands_and_decks, model_play)\n",
    "\t\t\taverage_gain = torch.mean(gains)\n",
    "\t\t\tprint(f\"epoch: {epoch}, loss: {loss.item()}, average gain: {average_gain}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_play \u001b[38;5;241m=\u001b[39m make_play_model()\n\u001b[1;32m      2\u001b[0m optimizer_play \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_play\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_play_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_play\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_play\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[141], line 15\u001b[0m, in \u001b[0;36mtrain_play_model\u001b[0;34m(model_play, optimizer_play, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m actions \u001b[38;5;241m=\u001b[39m model_play(initial_hands_and_decks)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39maverage_gain\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer_play\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/computer_graphic/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/computer_graphic/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/computer_graphic/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "model_play = make_play_model()\n",
    "optimizer_play = torch.optim.Adam(model_play.parameters(), lr=0.001)\n",
    "train_play_model(model_play, optimizer_play, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8479)\n",
      "tensor(0.4920)\n"
     ]
    }
   ],
   "source": [
    "print(F.cross_entropy(torch.tensor([0.0, 1.0]), torch.tensor([0.5346, 0.4654])))\n",
    "print(F.cross_entropy(torch.tensor([0.0, 1.0]), torch.tensor([0.1787, 0.8213])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "deck = np.array([N] * 13)\n",
    "bet_model = make_bet_model()\n",
    "bet = bet_model(torch.tensor(deck, dtype=torch.float32))\n",
    "print(bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "multinomial(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m deck \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([N] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m13\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m dealer_card \u001b[38;5;241m=\u001b[39m \u001b[43mdrawCard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeck\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m card1 \u001b[38;5;241m=\u001b[39m drawCard(deck)\n\u001b[1;32m      4\u001b[0m card2 \u001b[38;5;241m=\u001b[39m drawCard(deck)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mdrawCard\u001b[0;34m(deck)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrawCard\u001b[39m(deck):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\tDraw a card from the deck and return it\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \tcard \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      6\u001b[0m \tdeck[card] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m card\n",
      "\u001b[0;31mTypeError\u001b[0m: multinomial(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "deck = np.array([N] * 13)\n",
    "dealer_card = drawCard(deck)\n",
    "card1 = drawCard(deck)\n",
    "card2 = drawCard(deck)\n",
    "l = torch.tensor(np.concatenate([deck, [dealer_card, card1, card2]]), dtype=torch.float32)\n",
    "print(l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(valid_hands_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 20, 30, 40, -1])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_graphic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
